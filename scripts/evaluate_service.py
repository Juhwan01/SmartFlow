"""
ì„œë¹„ìŠ¤ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (ì‹¤ì œ Multi-Agent ì‹œìŠ¤í…œ ì‚¬ìš©)

ì‹¤ì œ ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Test ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import pandas as pd
import pickle
import json
from sklearn.metrics import mean_absolute_error
from loguru import logger
from datetime import datetime
from typing import Dict, List
from dataclasses import dataclass, asdict

# ì‹¤ì œ ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ import
from src.agents.process_monitor import ProcessMonitorAgent
from src.agents.ml_quality_predictor import MLQualityCascadePredictor, MLQualityPrediction
from src.agents.negotiation_agent import NegotiationAgent
from src.agents.coordinator import CoordinatorAgent, ProductionGoals
from src.data.sensor_simulator import PressSensorData
from config import settings


@dataclass
class SampleEvaluationResult:
    """ê°œë³„ ìƒ˜í”Œ í‰ê°€ ê²°ê³¼"""
    sample_id: int
    ground_truth: float
    baseline_prediction: float
    adjusted_prediction: float
    adjusted_ground_truth: float  # ì‹œë®¬ë ˆì´ì…˜ëœ ì‹¤ì œ ê°’
    is_anomaly: bool
    adjustment_applied: bool
    improvement: float
    meets_threshold_baseline: bool
    meets_threshold_adjusted: bool
    defect_prevented: bool
    applied_adjustments: Dict[str, float]
    decision_status: str
    iterations_used: int


class ServiceEvaluator:
    """ì‹¤ì œ Multi-Agent ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì„œë¹„ìŠ¤ í‰ê°€"""

    def __init__(
        self,
        quality_threshold: float = 0.90,
        cost_per_defect: float = 100.0,
        max_iterations: int = 3
    ):
        self.quality_threshold = quality_threshold
        self.cost_per_defect = cost_per_defect
        self.max_iterations = max_iterations

        # ì‹¤ì œ ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        logger.info("ì‹¤ì œ Multi-Agent ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self.process_monitor = ProcessMonitorAgent()

        # ML ëª¨ë¸ (load_test_dataì—ì„œ ë¡œë“œë¨)
        self.ml_model = None

        self.negotiation_agent = NegotiationAgent()
        self.coordinator = CoordinatorAgent(
            production_goals=ProductionGoals(
                target_quality=quality_threshold,
                max_cycle_time_increase=0.18,
                max_cost_increase=0.10
            )
        )

        self.test_data = None
        self.scaler = None
        self.feature_cols: List[str] = []
        self.lsl = None
        self.usl = None
        self.spec_span = 1.0
        self.sample_results: List[SampleEvaluationResult] = []

        # ë¬¼ë¦¬ì  íš¨ê³¼ ê³„ìˆ˜ (ground truth ì‹œë®¬ë ˆì´ì…˜ìš©)
        self.base_speed_coeff = 0.30
        self.base_current_coeff = 0.40
        self.base_pressure_coeff = 0.30

        logger.info("âœ… Multi-Agent ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_ml_model(self):
        """ML ëª¨ë¸ ë¡œë“œ"""
        model_path = Path("models/quality_predictor.pkl")
        try:
            with open(model_path, 'rb') as f:
                self.ml_model = pickle.load(f)
            logger.info(f"âœ… ML ëª¨ë¸ ë¡œë“œ: {model_path}")
        except Exception as e:
            logger.error(f"ML ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def predict_quality(self, raw_row: Dict[str, float]) -> MLQualityPrediction:
        """
        í’ˆì§ˆ ì˜ˆì¸¡ (ML ëª¨ë¸ ì§ì ‘ ì‚¬ìš©)

        Args:
            raw_row: ì›ë³¸ í”¼ì²˜ ë”•ì…”ë„ˆë¦¬

        Returns:
            MLQualityPrediction ê°ì²´
        """
        # í”¼ì²˜ ë°°ì—´ ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§
        features_raw = np.array([[raw_row[col] for col in self.feature_cols]])
        features_scaled = self.scaler.transform(features_raw)

        # ì˜ˆì¸¡
        predicted_strength = float(self.ml_model.predict(features_scaled)[0])

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (config ê¸°ë°˜ LSL/Target/USL)
        # LSL~Target: 0~90ì , Target~USL: 90~100ì 
        # LSL ë¯¸ë§Œì´ë‚˜ USL ì´ˆê³¼ë„ ì ìˆ˜ ë¶€ì—¬ (ìŒìˆ˜/100ì  ì´ˆê³¼ ê°€ëŠ¥ â†’ clip)
        if predicted_strength >= self.target:
            # Target ì´ìƒ: 90~100ì  (USL ì´ˆê³¼ ì‹œ 100ì  ì´ìƒ)
            predicted_quality_score = 0.9 + 0.1 * (predicted_strength - self.target) / (self.usl - self.target)
        else:
            # Target ë¯¸ë§Œ: 0~90ì  (LSL ë¯¸ë§Œ ì‹œ 0ì  ë¯¸ë§Œ)
            predicted_quality_score = 0.9 * (predicted_strength - self.lsl) / (self.target - self.lsl)

        predicted_quality_score = float(np.clip(predicted_quality_score, 0.0, 1.0))
        
        # ë””ë²„ê¹…: 0ì ì¸ ê²½ìš° ë¡œê·¸
        if predicted_quality_score <= 0.01:
            logger.warning(
                f"í’ˆì§ˆ ì ìˆ˜ 0%: strength={predicted_strength:.2f} < LSL={self.lsl:.2f} "
                f"(Target={self.target:.2f}, USL={self.usl:.2f})"
            )

        # ê°•ë„ ì €í•˜ìœ¨ (target ê¸°ì¤€)
        strength_degradation = max(0, (self.target - predicted_strength) / self.target * 100)

        # ìœ„í—˜ ìˆ˜ì¤€
        if predicted_quality_score >= self.quality_threshold:
            risk_level = "low"
        elif predicted_quality_score >= self.quality_threshold - 0.05:
            risk_level = "medium"
        elif predicted_quality_score >= self.quality_threshold - 0.15:
            risk_level = "high"
        else:
            risk_level = "critical"

        return MLQualityPrediction(
            predicted_strength=predicted_strength,
            predicted_quality_score=predicted_quality_score,
            strength_degradation_pct=strength_degradation,
            confidence=0.92,
            risk_level=risk_level,
            baseline_strength=self.target,
            model_used="XGBoost",
            recommendation="ìë™ í‰ê°€"
        )

    def load_test_data(self):
        """Test ë°ì´í„° ë¡œë“œ"""
        logger.info("=" * 70)
        logger.info("Test ë°ì´í„° ë¡œë“œ ì¤‘...")

        test_path = Path("data/test_set.csv")
        if not test_path.exists():
            raise FileNotFoundError(f"Test ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {test_path}")

        self.test_data = pd.read_csv(test_path)
        target_col = "welding_strength"
        self.feature_cols = [col for col in self.test_data.columns if col != target_col]

        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        scaler_path = Path("models/scaler.pkl")
        with open(scaler_path, 'rb') as f:
            self.scaler = pickle.load(f)

        # ML ëª¨ë¸ ë¡œë“œ (feature_cols ì„¤ì • í›„)
        self._load_ml_model()

        # í’ˆì§ˆ ìŠ¤í™ - configì—ì„œ ë¡œë“œ (ì—…ê³„ í‘œì¤€ ê³ ì •ê°’)
        self.lsl = settings.welding_strength_lsl
        self.usl = settings.welding_strength_usl
        self.target = settings.welding_strength_target
        self.spec_span = self.usl - self.lsl

        # ì‹¤ì œ ë°ì´í„°ì™€ ë¹„êµ
        target_values = self.test_data[target_col].values
        defects_count = np.sum((target_values < self.lsl) | (target_values > self.usl))
        defects_pct = defects_count / len(target_values) * 100
        
        logger.info(f"   í’ˆì§ˆ ê¸°ì¤€ (config): LSL={self.lsl:.2f}, Target={self.target:.2f}, USL={self.usl:.2f}")
        logger.info(f"   ì‹¤ì œ ë¶ˆëŸ‰ ìƒ˜í”Œ: {defects_count}/{len(target_values)} ({defects_pct:.1f}%)")
        
        below_target = np.sum(target_values < self.target)
        logger.info(f"   Target ë¯¸ë§Œ: {below_target}/{len(target_values)} ({below_target/len(target_values):.1%})")

        # ì´ìƒ ê°ì§€ë¥¼ ìœ„í•œ í†µê³„ê°’ ê³„ì‚° (1-2% ê°ì§€ ìˆ˜ì¤€)
        self.feature_stats = {}
        for col in self.feature_cols:
            values = self.test_data[col].values
            self.feature_stats[col] = {
                'mean': float(np.mean(values)),
                'std': float(np.std(values)),
                'q01': float(np.percentile(values, 1)),   # 1% í•˜í•œ
                'q99': float(np.percentile(values, 99)),  # 99% ìƒí•œ
                'q05': float(np.percentile(values, 5)),
                'q95': float(np.percentile(values, 95))
            }

        logger.info(f"âœ… Test ë°ì´í„°: {len(self.test_data)} samples")
        logger.info(f"   LSL={self.lsl:.4f}, USL={self.usl:.4f}")
        logger.info("=" * 70)

    def detect_anomaly(self, raw_row: Dict[str, float]) -> bool:
        """
        ì´ìƒ ê°ì§€ (MVP ì‹œë‚˜ë¦¬ì˜¤: 2ë‹¨ê³„ cascade detection)

        1ë‹¨ê³„: í”„ë ˆìŠ¤ ê³µì • ì´ìƒ ê°ì§€ (1ì°¨ í•„í„°)
        2ë‹¨ê³„: ìš©ì ‘ í’ˆì§ˆ ì˜í–¥ ì˜ˆì¸¡ (cascade effect)

        Args:
            raw_row: ì›ë³¸ í”¼ì²˜ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì´ìƒ ì—¬ë¶€ (ì¡°ì • í•„ìš” ì‹œ True)
        """
        # ========================================
        # 1ë‹¨ê³„: í”„ë ˆìŠ¤ ê³µì • ì´ìƒ ê°ì§€ (MVP 1ì°¨ í•„í„°)
        # ========================================
        press_thickness = raw_row.get('press_thickness', 0.0)
        press_anomaly = self.process_monitor.check_press_data_anomaly(press_thickness)

        if not press_anomaly:
            # í”„ë ˆìŠ¤ ì •ìƒì´ë©´ ì¡°ì • ë¶ˆí•„ìš” (ì¦‰ì‹œ ë°˜í™˜)
            return False

        # ========================================
        # 2ë‹¨ê³„: ìš©ì ‘ í’ˆì§ˆ ì˜í–¥ ì˜ˆì¸¡ (cascade effect)
        # ========================================
        # ML ëª¨ë¸ë¡œ ìµœì¢… í’ˆì§ˆ ì˜ˆì¸¡ (í”„ë ˆìŠ¤ ì´ìƒì´ ìš©ì ‘ì— ë¯¸ì¹  ì˜í–¥)
        prediction = self.predict_quality(raw_row)

        # í’ˆì§ˆ ì €í•˜ ì˜ˆìƒë˜ë©´ ì¡°ì • í•„ìš”
        return self.process_monitor.is_anomaly_detected(
            predicted_strength=prediction.predicted_strength,
            predicted_quality_score=prediction.predicted_quality_score
        )

    def _meets_quality_spec(self, value: float) -> bool:
        """í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± ì—¬ë¶€"""
        return self.lsl <= value <= self.usl

    def _inverse_scale_row(self, scaled_features: np.ndarray) -> Dict[str, float]:
        """ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¥¼ ì›ë³¸ ê°’ìœ¼ë¡œ ë³€í™˜"""
        raw_values = self.scaler.inverse_transform(scaled_features)[0]
        return {col: float(raw_values[i]) for i, col in enumerate(self.feature_cols)}

    def _scale_row(self, raw_row: Dict[str, float]) -> np.ndarray:
        """ì›ë³¸ ê°’ì„ ìŠ¤ì¼€ì¼ë§"""
        ordered = [raw_row[col] for col in self.feature_cols]
        return self.scaler.transform(np.array([ordered]))

    def _apply_adjustments(
        self,
        raw_row: Dict[str, float],
        adjustments: Dict[str, float]
    ) -> Dict[str, float]:
        """ì¡°ì •ê°’ ì ìš©"""
        adjusted = raw_row.copy()

        # íŒŒë¼ë¯¸í„° ë§¤í•‘
        param_mapping = {
            "welding_speed": "welding_temp3",
            "current": "welding_temp1",
            "pressure": "welding_pressure"
        }

        for adj_key, feature_name in param_mapping.items():
            if adj_key in adjustments and feature_name in adjusted:
                adjusted[feature_name] *= (1 + adjustments[adj_key])

        # íŒŒìƒ í”¼ì²˜ ì¬ê³„ì‚°
        if {"welding_temp1", "welding_temp3"}.issubset(adjusted):
            denom = adjusted["welding_temp3"] if adjusted["welding_temp3"] != 0 else 1e-5
            adjusted["heat_input_proxy"] = adjusted["welding_temp1"] / denom

        return adjusted

    def _simulate_ground_truth_effect(
        self,
        original_gt: float,
        adjustments: Dict[str, float]
    ) -> float:
        """ì¡°ì •ì´ ì‹¤ì œ í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì‹œë®¬ë ˆì´ì…˜"""
        if not adjustments:
            return original_gt

        speed_change = adjustments.get("welding_speed", 0)
        current_change = adjustments.get("current", 0)
        pressure_change = adjustments.get("pressure", 0)

        # ë¬¼ë¦¬ì  íš¨ê³¼ ê³„ì‚°
        strength_change_pct = (
            -speed_change * self.base_speed_coeff +
            current_change * self.base_current_coeff +
            pressure_change * self.base_pressure_coeff
        )

        adjusted_gt = original_gt * (1 + strength_change_pct)
        return float(np.clip(adjusted_gt, self.lsl - 0.5, self.usl + 0.5))

    def evaluate_samples(self, sample_size: int = None):
        """
        Test ìƒ˜í”Œ í‰ê°€ (ì‹¤ì œ Multi-Agent ì‹œìŠ¤í…œ ì‚¬ìš©)
        
        Args:
            sample_size: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” ì‹¤ì œ Multi-Agent ì‹œìŠ¤í…œìœ¼ë¡œ í‰ê°€ ì‹œì‘")
        logger.info("=" * 70)

        target_col = "welding_strength"
        
        # ìƒ˜í”Œ í¬ê¸° ê²°ì •
        if sample_size is not None and sample_size < len(self.test_data):
            logger.info(f"âš ï¸  ìƒ˜í”Œ í¬ê¸° ì œí•œ: {sample_size}/{len(self.test_data)} (ë¹ ë¥¸ í‰ê°€)")
            eval_data = self.test_data.head(sample_size)
        else:
            eval_data = self.test_data
            
        X_test = eval_data[self.feature_cols].values
        y_test = eval_data[target_col].values

        for i in range(len(X_test)):
            features = X_test[i:i+1]
            ground_truth = y_test[i]
            raw_row = self._inverse_scale_row(features)

            # 1. Process Monitor: ì´ìƒ ê°ì§€ (í†µê³„ ê¸°ë°˜)
            is_anomaly = self.detect_anomaly(raw_row)

            # 2. ML Quality Predictor: í’ˆì§ˆ ì˜ˆì¸¡
            baseline_pred_obj = self.predict_quality(raw_row)
            baseline_pred = baseline_pred_obj.predicted_strength

            # ë””ë²„ê·¸: ì²˜ìŒ 10ê°œ ìƒ˜í”Œ ë¡œê¹… (MVP 2ë‹¨ê³„ êµ¬ì¡° í‘œì‹œ)
            if i < 10:
                press_thickness = raw_row.get('press_thickness', 0.0)
                press_anomaly = self.process_monitor.check_press_data_anomaly(press_thickness)
                logger.info(
                    f"ìƒ˜í”Œ {i}: press_thickness={press_thickness:.4f}mm, "
                    f"press_anomaly={press_anomaly}, "
                    f"pred_strength={baseline_pred:.4f}, "
                    f"quality_score={baseline_pred_obj.predicted_quality_score:.4f}, "
                    f"final_anomaly={is_anomaly}"
                )

            decision_status = "skipped"
            adjustment_applied = False
            adjusted_pred = baseline_pred
            applied_adjustments = {}
            iterations_used = 0

            # 3. ì´ìƒì´ ê°ì§€ë˜ë©´ Multi-Agent í˜‘ìƒ ì‹œì‘
            if is_anomaly:
                working_raw = raw_row
                current_pred_obj = baseline_pred_obj

                for iteration in range(self.max_iterations):
                    # í’ˆì§ˆ ì ìˆ˜ê°€ ëª©í‘œì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
                    if current_pred_obj.predicted_quality_score >= self.quality_threshold:
                        break

                    # Negotiation Agent: RAG ê¸°ë°˜ ì¡°ì • ì œì•ˆ
                    current_issue = f"í’ˆì§ˆ ì €í•˜ ê°ì§€: í’ˆì§ˆ ì ìˆ˜ {current_pred_obj.predicted_quality_score:.2%} (ëª©í‘œ: {self.quality_threshold:.0%})"

                    try:
                        proposal = self.negotiation_agent.analyze_situation_and_propose(
                            current_issue=current_issue,
                            prediction=current_pred_obj,
                            process_data=working_raw
                        )
                    except Exception as e:
                        logger.warning(f"Negotiation Agent ì˜¤ë¥˜: {e}")
                        break

                    # Coordinator: ìŠ¹ì¸/ë°˜ë ¤
                    current_quality_score = max(0.0, min(1.0, current_pred_obj.predicted_quality_score))
                    decision = self.coordinator.evaluate_proposal(
                        proposal=proposal,
                        current_quality_score=current_quality_score
                    )

                    decision_status = decision.status

                    if decision.status in ["approved", "conditional_approved"]:
                        # ì¡°ì • ì ìš©
                        working_raw = self._apply_adjustments(working_raw, proposal.adjustments)
                        working_features = self._scale_row(working_raw)
                        current_pred_obj = self.predict_quality(working_raw)

                        applied_adjustments = proposal.adjustments
                        adjustment_applied = True
                        adjusted_pred = current_pred_obj.predicted_strength
                        iterations_used += 1
                    else:
                        # ë°˜ë ¤ë˜ë©´ ì¢…ë£Œ
                        break

            # Ground truth ì‹œë®¬ë ˆì´ì…˜
            adjusted_ground_truth = self._simulate_ground_truth_effect(
                ground_truth,
                applied_adjustments
            )

            # ê²°ê³¼ ê¸°ë¡
            meets_baseline = self._meets_quality_spec(ground_truth)
            meets_adjusted = self._meets_quality_spec(adjusted_ground_truth)
            defect_prevented = (not meets_baseline) and meets_adjusted
            improvement = adjusted_pred - baseline_pred

            result = SampleEvaluationResult(
                sample_id=i,
                ground_truth=ground_truth,
                baseline_prediction=baseline_pred,
                adjusted_prediction=adjusted_pred,
                adjusted_ground_truth=adjusted_ground_truth,
                is_anomaly=is_anomaly,
                adjustment_applied=adjustment_applied,
                improvement=improvement,
                meets_threshold_baseline=meets_baseline,
                meets_threshold_adjusted=meets_adjusted,
                defect_prevented=defect_prevented,
                applied_adjustments=applied_adjustments,
                decision_status=decision_status,
                iterations_used=iterations_used
            )
            self.sample_results.append(result)

            if (i + 1) % 100 == 0:
                logger.info(f"  ì§„í–‰: {i+1}/{len(X_test)} ìƒ˜í”Œ ì™„ë£Œ")

        logger.info("=" * 70)
        logger.info("âœ… í‰ê°€ ì™„ë£Œ")

    def calculate_and_report_metrics(self):
        """ì§€í‘œ ê³„ì‚° ë° ë¦¬í¬íŠ¸"""
        total_samples = len(self.sample_results)
        anomalies_detected = sum(1 for r in self.sample_results if r.is_anomaly)
        adjustments_made = sum(1 for r in self.sample_results if r.adjustment_applied)

        defects_before = sum(1 for r in self.sample_results if not r.meets_threshold_baseline)
        defects_after = sum(1 for r in self.sample_results if not r.meets_threshold_adjusted)
        defects_prevented = sum(1 for r in self.sample_results if r.defect_prevented)

        defect_reduction_rate = defects_prevented / defects_before if defects_before > 0 else 0.0
        quality_recovery_rate = defects_prevented / anomalies_detected if anomalies_detected > 0 else 0.0

        cost_saving = defects_prevented * self.cost_per_defect

        successful_adjustments = sum(
            1 for r in self.sample_results
            if r.adjustment_applied and (r.defect_prevented or r.meets_threshold_adjusted)
        )
        adjustment_success_rate = successful_adjustments / adjustments_made if adjustments_made > 0 else 0.0

        quality_gains = [
            r.adjusted_ground_truth - r.ground_truth
            for r in self.sample_results if r.adjustment_applied
        ]
        avg_improvement = float(np.mean(quality_gains)) if quality_gains else 0.0

        # ë¦¬í¬íŠ¸ ì¶œë ¥
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š ì‹¤ì œ Multi-Agent ì‹œìŠ¤í…œ í‰ê°€ ê²°ê³¼")
        logger.info("=" * 70)
        logger.info(f"\n[ê¸°ë³¸ í†µê³„]")
        logger.info(f"  ì´ ìƒ˜í”Œ ìˆ˜: {total_samples}")
        logger.info(f"  ê°ì§€ëœ ì´ìƒ: {anomalies_detected} ({anomalies_detected/total_samples:.1%})")
        logger.info(f"  ì ìš©ëœ ì¡°ì •: {adjustments_made}")
        logger.info(f"\n[ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸]")
        logger.info(f"  ì¡°ì • ì „ ë¶ˆëŸ‰: {defects_before}ê°œ")
        logger.info(f"  ì¡°ì • í›„ ë¶ˆëŸ‰: {defects_after}ê°œ")
        logger.info(f"  ë°©ì§€ëœ ë¶ˆëŸ‰: {defects_prevented}ê°œ")
        logger.info(f"  ë¶ˆëŸ‰ ê°ì†Œìœ¨: {defect_reduction_rate:.1%}")
        logger.info(f"  í’ˆì§ˆ íšŒë³µìœ¨: {quality_recovery_rate:.1%}")
        logger.info(f"  ë¹„ìš© ì ˆê°ì•¡: ${cost_saving:,.2f}")
        logger.info(f"\n[ì¡°ì • íš¨ê³¼ì„±]")
        logger.info(f"  ì„±ê³µí•œ ì¡°ì •: {successful_adjustments}/{adjustments_made}")
        logger.info(f"  ì¡°ì • ì„±ê³µë¥ : {adjustment_success_rate:.1%}")
        logger.info(f"  ì¡°ì •ë‹¹ í‰ê·  ê°œì„ : {avg_improvement:.4f}")
        logger.info("=" * 70)

        # ê²°ê³¼ ì €ì¥
        results = {
            "evaluation_date": datetime.now().isoformat(),
            "total_samples": total_samples,
            "anomalies_detected": anomalies_detected,
            "adjustments_made": adjustments_made,
            "defects_prevented": defects_prevented,
            "defect_reduction_rate": float(defect_reduction_rate),
            "quality_recovery_rate": float(quality_recovery_rate),
            "cost_saving": float(cost_saving),
            "successful_adjustments": successful_adjustments,
            "adjustment_success_rate": float(adjustment_success_rate),
            "avg_improvement": float(avg_improvement)
        }

        output_dir = Path("models")
        output_dir.mkdir(parents=True, exist_ok=True)

        results_path = output_dir / "service_evaluation_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥: {results_path}")

        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        report_path = output_dir / "service_evaluation_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("SmartFlow Multi-Agent ì‹œìŠ¤í…œ í‰ê°€ ë¦¬í¬íŠ¸\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Test ìƒ˜í”Œ ìˆ˜: {total_samples}\n\n")
            f.write("=" * 70 + "\n")
            f.write("ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸\n")
            f.write("=" * 70 + "\n")
            f.write(f"ê°ì§€ëœ ì´ìƒ: {anomalies_detected}ê±´\n")
            f.write(f"ë°©ì§€ëœ ë¶ˆëŸ‰: {defects_prevented}ê±´\n")
            f.write(f"ë¶ˆëŸ‰ ê°ì†Œìœ¨: {defect_reduction_rate:.1%}\n")
            f.write(f"í’ˆì§ˆ íšŒë³µìœ¨: {quality_recovery_rate:.1%}\n")
            f.write(f"ë¹„ìš© ì ˆê°ì•¡: ${cost_saving:,.2f}\n\n")
            f.write("=" * 70 + "\n")
            f.write("ì¡°ì • ì‹œìŠ¤í…œ íš¨ê³¼ì„±\n")
            f.write("=" * 70 + "\n")
            f.write(f"ì ìš©ëœ ì¡°ì •: {adjustments_made}íšŒ\n")
            f.write(f"ì„±ê³µí•œ ì¡°ì •: {successful_adjustments}íšŒ\n")
            f.write(f"ì¡°ì • ì„±ê³µë¥ : {adjustment_success_rate:.1%}\n")
            f.write(f"ì¡°ì •ë‹¹ í‰ê·  ê°œì„ : {avg_improvement:.4f}\n\n")
            f.write("=" * 70 + "\n")

        logger.info(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SmartFlow Multi-Agent ì‹œìŠ¤í…œ í‰ê°€")
    parser.add_argument(
        "--sample-size",
        type=int,
        default=None,
        help="í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: ì „ì²´ ë°ì´í„°)"
    )
    args = parser.parse_args()
    
    print("\n" + "=" * 70)
    print("ğŸ”¬ SmartFlow Multi-Agent ì‹œìŠ¤í…œ í‰ê°€")
    print("=" * 70)
    print("âš ï¸  ì‹¤ì œ ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Test ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.")
    if args.sample_size:
        print(f"   ìƒ˜í”Œ í¬ê¸°: {args.sample_size}")
    print("=" * 70)

    try:
        evaluator = ServiceEvaluator(
            quality_threshold=0.90,
            cost_per_defect=100.0,
            max_iterations=3
        )

        evaluator.load_test_data()
        evaluator.evaluate_samples(sample_size=args.sample_size)
        evaluator.calculate_and_report_metrics()

        print("\n" + "=" * 70)
        print("âœ… í‰ê°€ ì™„ë£Œ!")
        print("=" * 70)

    except FileNotFoundError as e:
        logger.error(f"âŒ {e}")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
