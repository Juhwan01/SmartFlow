"""
ML ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ìˆ˜ì •ë²„ì „ - XGBoost 2.0+ í˜¸í™˜)

1. XGBoost 2.0+ í˜¸í™˜ì„± ì™„ë²½ ìˆ˜ì • (eval_metric, early_stopping_rounds ìœ„ì¹˜ ë³€ê²½)
2. ë¹„ì¦ˆë‹ˆìŠ¤ KPI (MAE, MAPE) ì¤‘ì‹¬ì˜ í•™ìŠµ ë° í‰ê°€
3. ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ Feature Engineering ì ìš©
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import pandas as pd
import pickle
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
from loguru import logger
import json

from src.data.data_preprocessing import ManufacturingDataProcessor


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµê¸°"""

    def __init__(self):
        self.processor = ManufacturingDataProcessor()
        self.model = None
        self.metrics = {}
        self.scaler = MinMaxScaler() # ìì²´ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©

    def _calculate_mape(self, y_true, y_pred):
        """MAPE ê³„ì‚° (ì •ë°€ë„ ì§€í‘œ)"""
        threshold = 0.1
        mask = np.abs(y_true) > threshold
        if mask.sum() == 0:
            return 0.0
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    def feature_engineering(self, df):
        """ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë³€ìˆ˜ ì¶”ê°€"""
        df_fe = df.copy()
        
        # 1. ìš©ì ‘ ì…ì—´ëŸ‰ (Heat Input) ìœ ì‚¬ ë³€ìˆ˜
        df_fe['heat_input_proxy'] = df_fe['welding_temp1'] / (df_fe['welding_temp3'] + 1e-5)

        # 2. ì••ë ¥ê³¼ ì˜¨ë„ì˜ ìƒí˜¸ì‘ìš©
        df_fe['pressure_x_temp2'] = df_fe['welding_pressure'] * df_fe['welding_temp2']
        
        # 3. ì œì–´ ë³€ìˆ˜ í•©ê³„
        df_fe['total_control'] = df_fe['welding_control1'] + df_fe['welding_control2']

        # 4. í”„ë ˆìŠ¤ ê³µì •ì˜ ë©´ì /ë¶€í”¼ ìœ ì‚¬ ë³€ìˆ˜
        df_fe['press_volume_proxy'] = df_fe['press_thickness'] * df_fe['press_measurement1']

        return df_fe

    def train_xgboost(
        self,
        n_estimators: int = 2000,
        max_depth: int = 8,
        learning_rate: float = 0.02
    ):
        logger.info("=" * 70)
        logger.info("XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘ (KPI: MAE ìµœì†Œí™”)")
        logger.info("=" * 70)

        # 1. ë°ì´í„° ë¡œë“œ
        df = self.processor.create_mapped_dataset()
        target_col = "welding_strength"

        # 2. 0ê°’ ë°ì´í„° í•„í„°ë§
        initial_len = len(df)
        df = df[df[target_col] > 0.1].copy()
        logger.info(f"ë°ì´í„° í•„í„°ë§: {initial_len} -> {len(df)} (ìœ íš¨ ê³µì • ë°ì´í„°ë§Œ ì‚¬ìš©)")

        # 3. Feature Engineering ì ìš©
        df_fe = self.feature_engineering(df)
        feature_cols = [col for col in df_fe.columns if col != target_col]

        X = df_fe[feature_cols].values
        y = df_fe[target_col].values

        # 4. ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scaler.fit_transform(X)

        # 5. Train/Validation/Test ë¶„ë¦¬ (70/15/15)
        # Step 1: Train+Val / Test ë¶„ë¦¬
        X_trainval, X_test, y_trainval, y_test = train_test_split(
            X_scaled, y, test_size=0.15, random_state=42
        )

        # Step 2: Train / Validation ë¶„ë¦¬
        X_train, X_val, y_train, y_val = train_test_split(
            X_trainval, y_trainval, test_size=0.176, random_state=42  # 0.176 â‰ˆ 15/(70+15)
        )

        logger.info(f"ë°ì´í„° ë¶„í• : Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")

        # 6. Test ë°ì´í„° ì €ì¥ (ìµœì¢… í‰ê°€ìš©)
        test_df = pd.DataFrame(X_test, columns=feature_cols)
        test_df[target_col] = y_test
        test_path = Path("data/test_set.csv")
        test_path.parent.mkdir(parents=True, exist_ok=True)
        test_df.to_csv(test_path, index=False)
        logger.info(f"âœ… Test ë°ì´í„° ì €ì¥: {test_path} (ìµœì¢… í‰ê°€ìš©, ì ˆëŒ€ ì¬í•™ìŠµ ê¸ˆì§€)")

        # 6-1. Sample Weighting ê³„ì‚° (ë¶ˆëŸ‰í’ˆ ê°•ì¡° í•™ìŠµ)
        # ===================================================================
        # ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘: ë¶ˆëŸ‰ ìƒ˜í”Œì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        # ì°¸ê³ : 2024 ì œì¡° ë¶ˆëŸ‰ ê°ì§€ ì—°êµ¬ (MDPI Sensors)
        # ===================================================================
        SETPOINT = 12.0500
        LSL = SETPOINT * 0.9  # 10.8450
        USL = SETPOINT * 1.1  # 13.2550

        # Train ë°ì´í„°ì—ì„œ ë¶ˆëŸ‰ ìƒ˜í”Œ ì‹ë³„
        train_defects = (y_train < LSL) | (y_train > USL)
        num_defects = train_defects.sum()
        num_normal = len(y_train) - num_defects

        # ê°€ì¤‘ì¹˜ ê³„ì‚°: ë¶ˆëŸ‰ ìƒ˜í”Œì— ì •ìƒ/ë¶ˆëŸ‰ ë¹„ìœ¨ë§Œí¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        if num_defects > 0:
            defect_weight = num_normal / num_defects
        else:
            defect_weight = 1.0

        # Sample weights ìƒì„±
        sample_weights = np.ones(len(y_train))
        sample_weights[train_defects] = defect_weight

        logger.info(f"\në¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘:")
        logger.info(f"  - ì •ìƒ ìƒ˜í”Œ: {num_normal}ê°œ (ê°€ì¤‘ì¹˜: 1.0)")
        logger.info(f"  - ë¶ˆëŸ‰ ìƒ˜í”Œ: {num_defects}ê°œ (ê°€ì¤‘ì¹˜: {defect_weight:.1f})")
        logger.info(f"  - ë¶ˆëŸ‰ë¥ : {num_defects/len(y_train)*100:.2f}%")

        # 7. ëª¨ë¸ í•™ìŠµ (Validation ë°ì´í„°ë¡œ Early Stopping)
        self.model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            min_child_weight=1,
            max_delta_step=1,  # ê·¹ë‹¨ê°’ í•™ìŠµ ê°œì„ 
            subsample=0.8,
            colsample_bytree=0.8,
            early_stopping_rounds=50,
            eval_metric='mae',
            objective='reg:squarederror',
            random_state=42,
            n_jobs=-1
        )

        logger.info("\nëª¨ë¸ í•™ìŠµ ì¤‘ (ë¶ˆëŸ‰ ìƒ˜í”Œ ê°•ì¡° í•™ìŠµ + Validation Early Stopping)...")

        # Validation ë°ì´í„°ë¡œ early stopping (sample_weight ì ìš©)
        self.model.fit(
            X_train, y_train,
            sample_weight=sample_weights,  # ë¶ˆëŸ‰ ìƒ˜í”Œ ê°•ì¡°!
            eval_set=[(X_val, y_val)],
            verbose=False
        )

        # 8. ì˜ˆì¸¡ ë° í‰ê°€ (Train, Validationë§Œ - TestëŠ” evaluate_final.pyì—ì„œ)
        y_pred_train = self.model.predict(X_train)
        y_pred_val = self.model.predict(X_val)

        self.metrics = {
            "train": {
                "r2": r2_score(y_train, y_pred_train),
                "mae": mean_absolute_error(y_train, y_pred_train),
                "rmse": np.sqrt(mean_squared_error(y_train, y_pred_train)),
                "mape": self._calculate_mape(y_train, y_pred_train)
            },
            "validation": {
                "r2": r2_score(y_val, y_pred_val),
                "mae": mean_absolute_error(y_val, y_pred_val),
                "rmse": np.sqrt(mean_squared_error(y_val, y_pred_val)),
                "mape": self._calculate_mape(y_val, y_pred_val)
            },
            "test": {
                "note": "Test í‰ê°€ëŠ” scripts/evaluate_final.pyì—ì„œ ë‹¨ 1íšŒë§Œ ìˆ˜í–‰",
                "test_set_path": "data/test_set.csv"
            }
        }

        # ê²°ê³¼ ì¶œë ¥
        logger.info("\n" + "=" * 70)
        logger.info("í•™ìŠµ ê²°ê³¼ (Validation Set)")
        logger.info("=" * 70)
        logger.info(f"âœ… MAE (í‰ê·  ì˜¤ì°¨): {self.metrics['validation']['mae']:.4f} (ëª©í‘œ: < 0.2)")
        logger.info(f"âœ… MAPE (ì˜¤ì°¨ìœ¨)  : {self.metrics['validation']['mape']:.2f}%  (ëª©í‘œ: < 2%)")
        logger.info(f"â„¹ï¸  RÂ² Score     : {self.metrics['validation']['r2']:.4f}")
        logger.info("=" * 70)
        logger.info("âš ï¸  Test ë°ì´í„° í‰ê°€ëŠ” scripts/evaluate_final.pyì—ì„œ ë‹¨ 1íšŒë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        logger.info("=" * 70)

        # Feature Importance
        logger.info("\n[í•µì‹¬ ì˜í–¥ ë³€ìˆ˜ - Top 5]")
        feature_importance = self.model.feature_importances_
        importance_dict = dict(zip(feature_cols, feature_importance))
        sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_importance[:5], 1):
            logger.info(f"  {i}. {feature}: {importance:.4f}")

        return self.model, self.metrics

    def save_model(self, model_path: str = "models/quality_predictor.pkl"):
        """ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ ì €ì¥"""
        if self.model is None:
            return

        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        with open("models/scaler.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)

        with open("models/metrics.json", 'w') as f:
            json.dump(self.metrics, f, indent=2)
            
        with open("models/variable_mapping.json", 'w', encoding='utf-8') as f:
            json.dump(self.processor.variable_mapping, f, indent=2, ensure_ascii=False)
            
        logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

def main():
    logger.info("SmartFlow ML Model Training (Business KPI Optimized)")

    trainer = ModelTrainer()
    model, metrics = trainer.train_xgboost()
    trainer.save_model()

    print("\n" + "=" * 70)
    print("ğŸ¯ í•™ìŠµ ì„±ëŠ¥ ìš”ì•½ (Validation Set)")
    print("=" * 70)

    mae_score = metrics['validation']['mae']
    mape_score = metrics['validation']['mape']

    print(f"âœ… Validation MAE  : {mae_score:.4f} (ëª©í‘œ: < 0.2)")
    print(f"âœ… Validation MAPE : {mape_score:.2f}% (ëª©í‘œ: < 2.0%)")
    print(f"â„¹ï¸  Validation RÂ²   : {metrics['validation']['r2']:.4f}")

    if mae_score < 0.2:
        print("\nğŸ‰ ëª©í‘œ ë‹¬ì„±! í˜„ì¥ íˆ¬ì… ê°€ëŠ¥í•œ ì´ˆì •ë°€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        print("   (í‰ê·  ì˜¤ì°¨ 0.2 ë¯¸ë§Œìœ¼ë¡œ í’ˆì§ˆ ì œì–´ ê°€ëŠ¥)")
    else:
        print(f"\nâš ï¸  ì¶”ê°€ íŠœë‹ í•„ìš” (í˜„ì¬ ì˜¤ì°¨: {mae_score:.4f})")

    print("=" * 70)
    print("âš ï¸  ìµœì¢… Test í‰ê°€ëŠ” 'python scripts/evaluate_final.py'ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”.")
    print("=" * 70)

if __name__ == "__main__":
    main()