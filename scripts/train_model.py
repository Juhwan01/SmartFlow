"""
ML ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ìˆ˜ì •ë²„ì „ - XGBoost 2.0+ í˜¸í™˜)

1. XGBoost 2.0+ í˜¸í™˜ì„± ì™„ë²½ ìˆ˜ì • (eval_metric, early_stopping_rounds ìœ„ì¹˜ ë³€ê²½)
2. ë¹„ì¦ˆë‹ˆìŠ¤ KPI (MAE, MAPE) ì¤‘ì‹¬ì˜ í•™ìŠµ ë° í‰ê°€
3. ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ Feature Engineering ì ìš©
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import pandas as pd
import pickle
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
from loguru import logger
import json

from src.data.data_preprocessing import ManufacturingDataProcessor


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµê¸°"""

    def __init__(self):
        self.processor = ManufacturingDataProcessor()
        self.model = None
        self.metrics = {}
        self.scaler = MinMaxScaler() # ìì²´ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©

    def _calculate_mape(self, y_true, y_pred):
        """MAPE ê³„ì‚° (ì •ë°€ë„ ì§€í‘œ)"""
        threshold = 0.1
        mask = np.abs(y_true) > threshold
        if mask.sum() == 0:
            return 0.0
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    def feature_engineering(self, df):
        """ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë³€ìˆ˜ ì¶”ê°€"""
        df_fe = df.copy()
        
        # 1. ìš©ì ‘ ì…ì—´ëŸ‰ (Heat Input) ìœ ì‚¬ ë³€ìˆ˜
        df_fe['heat_input_proxy'] = df_fe['welding_temp1'] / (df_fe['welding_temp3'] + 1e-5)

        # 2. ì••ë ¥ê³¼ ì˜¨ë„ì˜ ìƒí˜¸ì‘ìš©
        df_fe['pressure_x_temp2'] = df_fe['welding_pressure'] * df_fe['welding_temp2']
        
        # 3. ì œì–´ ë³€ìˆ˜ í•©ê³„
        df_fe['total_control'] = df_fe['welding_control1'] + df_fe['welding_control2']

        # 4. í”„ë ˆìŠ¤ ê³µì •ì˜ ë©´ì /ë¶€í”¼ ìœ ì‚¬ ë³€ìˆ˜
        df_fe['press_volume_proxy'] = df_fe['press_thickness'] * df_fe['press_measurement1']

        return df_fe

    def train_xgboost(
        self,
        n_estimators: int = 2000,
        max_depth: int = 8,
        learning_rate: float = 0.02
    ):
        logger.info("=" * 70)
        logger.info("XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘ (KPI: MAE ìµœì†Œí™”)")
        logger.info("=" * 70)

        # 1. ë°ì´í„° ë¡œë“œ
        df = self.processor.create_mapped_dataset()
        target_col = "welding_strength"

        # 2. 0ê°’ ë°ì´í„° í•„í„°ë§
        initial_len = len(df)
        df = df[df[target_col] > 0.1].copy()
        logger.info(f"ë°ì´í„° í•„í„°ë§: {initial_len} -> {len(df)} (ìœ íš¨ ê³µì • ë°ì´í„°ë§Œ ì‚¬ìš©)")

        # 3. Feature Engineering ì ìš©
        df_fe = self.feature_engineering(df)
        feature_cols = [col for col in df_fe.columns if col != target_col]
        
        X = df_fe[feature_cols].values
        y = df_fe[target_col].values

        # 4. ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scaler.fit_transform(X)

        # 5. Train/Test ë¶„ë¦¬
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )

        # 6. ëª¨ë¸ í•™ìŠµ
        # [ìˆ˜ì •ë¨] eval_metricê³¼ early_stopping_roundsë¥¼ ìƒì„±ìë¡œ ì´ë™
        self.model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            min_child_weight=1,
            subsample=0.8,
            colsample_bytree=0.8,
            early_stopping_rounds=50,
            eval_metric='mae',           # <-- ì—¬ê¸°ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤
            objective='reg:squarederror',
            random_state=42,
            n_jobs=-1
        )

        logger.info("ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # [ìˆ˜ì •ë¨] fit í•¨ìˆ˜ì—ì„œ ë§¤ê°œë³€ìˆ˜ ì œê±°
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )

        # 7. ì˜ˆì¸¡ ë° í‰ê°€
        y_pred_train = self.model.predict(X_train)
        y_pred_test = self.model.predict(X_test)

        self.metrics = {
            "train": {
                "r2": r2_score(y_train, y_pred_train),
                "mae": mean_absolute_error(y_train, y_pred_train),
                "rmse": np.sqrt(mean_squared_error(y_train, y_pred_train)),
                "mape": self._calculate_mape(y_train, y_pred_train)
            },
            "test": {
                "r2": r2_score(y_test, y_pred_test),
                "mae": mean_absolute_error(y_test, y_pred_test),
                "rmse": np.sqrt(mean_squared_error(y_test, y_pred_test)),
                "mape": self._calculate_mape(y_test, y_pred_test)
            }
        }

        # ê²°ê³¼ ì¶œë ¥
        logger.info("\n" + "=" * 70)
        logger.info("í•™ìŠµ ê²°ê³¼ (Test Set)")
        logger.info("=" * 70)
        logger.info(f"âœ… MAE (í‰ê·  ì˜¤ì°¨): {self.metrics['test']['mae']:.4f} (ëª©í‘œ: < 0.2)")
        logger.info(f"âœ… MAPE (ì˜¤ì°¨ìœ¨)  : {self.metrics['test']['mape']:.2f}%  (ëª©í‘œ: < 2%)")
        logger.info(f"â„¹ï¸  RÂ² Score     : {self.metrics['test']['r2']:.4f}")
        logger.info("=" * 70)

        # Feature Importance
        logger.info("\n[í•µì‹¬ ì˜í–¥ ë³€ìˆ˜ - Top 5]")
        feature_importance = self.model.feature_importances_
        importance_dict = dict(zip(feature_cols, feature_importance))
        sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_importance[:5], 1):
            logger.info(f"  {i}. {feature}: {importance:.4f}")

        return self.model, self.metrics

    def save_model(self, model_path: str = "models/quality_predictor.pkl"):
        """ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ ì €ì¥"""
        if self.model is None:
            return

        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        with open("models/scaler.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)

        with open("models/metrics.json", 'w') as f:
            json.dump(self.metrics, f, indent=2)
            
        with open("models/variable_mapping.json", 'w', encoding='utf-8') as f:
            json.dump(self.processor.variable_mapping, f, indent=2, ensure_ascii=False)
            
        logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

def main():
    logger.info("SmartFlow ML Model Training (Business KPI Optimized)")
    
    trainer = ModelTrainer()
    model, metrics = trainer.train_xgboost()
    trainer.save_model()

    print("\n" + "=" * 70)
    print("ğŸ¯ ìµœì¢… ì„±ëŠ¥ ìš”ì•½")
    print("=" * 70)
    
    mae_score = metrics['test']['mae']
    mape_score = metrics['test']['mape']
    
    print(f"âœ… Test MAE  : {mae_score:.4f} (ëª©í‘œ: < 0.2)")
    print(f"âœ… Test MAPE : {mape_score:.2f}% (ëª©í‘œ: < 2.0%)")
    print(f"â„¹ï¸  Test RÂ²   : {metrics['test']['r2']:.4f}")

    if mae_score < 0.2:
        print("\nğŸ‰ ëª©í‘œ ë‹¬ì„±! í˜„ì¥ íˆ¬ì… ê°€ëŠ¥í•œ ì´ˆì •ë°€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        print("   (í‰ê·  ì˜¤ì°¨ 0.2 ë¯¸ë§Œìœ¼ë¡œ í’ˆì§ˆ ì œì–´ ê°€ëŠ¥)")
    else:
        print(f"\nâš ï¸  ì¶”ê°€ íŠœë‹ í•„ìš” (í˜„ì¬ ì˜¤ì°¨: {mae_score:.4f})")
    
    print("=" * 70)

if __name__ == "__main__":
    main()