"""
ML ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

XGBoostë¥¼ ì‚¬ìš©í•˜ì—¬ Stage1 â†’ Stage2 í’ˆì§ˆ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import pickle
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
from loguru import logger
import json

from src.data.data_preprocessing import ManufacturingDataProcessor


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµê¸°"""

    def __init__(self):
        self.processor = ManufacturingDataProcessor()
        self.model = None
        self.metrics = {}

    def _calculate_mape(self, y_true, y_pred):
        """MAPE ê³„ì‚° (0ê³¼ 0ì— ê°€ê¹Œìš´ ê°’ ì œì™¸)"""
        # ì ˆëŒ“ê°’ì´ ì„ê³„ê°’ë³´ë‹¤ í° ê°’ë§Œ ì‚¬ìš© (0ê³¼ 0ì— ê°€ê¹Œìš´ ê°’ ì œì™¸)
        threshold = 0.1  # Targetì˜ 1% ì´ìƒì¸ ê°’ë§Œ ì‚¬ìš©
        mask = np.abs(y_true) > threshold

        if mask.sum() == 0:
            return 0.0

        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape

    def train_xgboost(
        self,
        n_estimators: int = 100,
        max_depth: int = 6,
        learning_rate: float = 0.1
    ):
        """XGBoost ëª¨ë¸ í•™ìŠµ"""
        logger.info("=" * 70)
        logger.info("XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 70)

        # ë°ì´í„° ì¤€ë¹„
        X_train, X_test, y_train, y_test, scaler = self.processor.prepare_ml_dataset()

        # Target ë¶„í¬ í™•ì¸
        logger.info(f"Target (y_train) í†µê³„:")
        logger.info(f"  Min: {y_train.min():.4f}, Max: {y_train.max():.4f}")
        logger.info(f"  Mean: {y_train.mean():.4f}, Std: {y_train.std():.4f}")
        logger.info(f"  0ì— ê°€ê¹Œìš´ ê°’ (<0.1): {(np.abs(y_train) < 0.1).sum()}ê°œ ({(np.abs(y_train) < 0.1).sum() / len(y_train) * 100:.1f}%)")

        # ëª¨ë¸ ìƒì„±
        self.model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            objective='reg:squarederror',
            random_state=42,
            n_jobs=-1
        )

        # í•™ìŠµ
        logger.info("ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )

        # ì˜ˆì¸¡
        y_pred_train = self.model.predict(X_train)
        y_pred_test = self.model.predict(X_test)

        # í‰ê°€
        self.metrics = {
            "train": {
                "mae": mean_absolute_error(y_train, y_pred_train),
                "rmse": np.sqrt(mean_squared_error(y_train, y_pred_train)),
                "r2": r2_score(y_train, y_pred_train),
                "mape": self._calculate_mape(y_train, y_pred_train)
            },
            "test": {
                "mae": mean_absolute_error(y_test, y_pred_test),
                "rmse": np.sqrt(mean_squared_error(y_test, y_pred_test)),
                "r2": r2_score(y_test, y_pred_test),
                "mape": self._calculate_mape(y_test, y_pred_test)
            }
        }

        # ê²°ê³¼ ì¶œë ¥
        logger.info("\n" + "=" * 70)
        logger.info("í•™ìŠµ ê²°ê³¼")
        logger.info("=" * 70)
        logger.info(f"[Train Set]")
        logger.info(f"  MAE: {self.metrics['train']['mae']:.4f}")
        logger.info(f"  RMSE: {self.metrics['train']['rmse']:.4f}")
        logger.info(f"  RÂ²: {self.metrics['train']['r2']:.4f}")
        logger.info(f"  MAPE: {self.metrics['train']['mape']:.2f}%")

        logger.info(f"\n[Test Set]")
        logger.info(f"  MAE: {self.metrics['test']['mae']:.4f}")
        logger.info(f"  RMSE: {self.metrics['test']['rmse']:.4f}")
        logger.info(f"  RÂ²: {self.metrics['test']['r2']:.4f}")
        logger.info(f"  MAPE: {self.metrics['test']['mape']:.2f}%")
        logger.info("=" * 70)

        # Feature Importance
        feature_importance = self.model.feature_importances_
        feature_names = [
            col for col in self.processor.variable_mapping.keys()
            if col != "welding_strength"
        ]

        logger.info("\n[Feature Importance - Top 5]")
        importance_dict = dict(zip(feature_names, feature_importance))
        sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_importance[:5], 1):
            logger.info(f"  {i}. {feature}: {importance:.4f}")

        return self.model, self.metrics

    def save_model(self, model_path: str = "models/quality_predictor.pkl"):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ ì €ì¥
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

        # Scaler ì €ì¥
        self.processor.save_scaler("models/scaler.pkl")

        # ë©”íŠ¸ë¦­ ì €ì¥
        metrics_path = "models/metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(self.metrics, f, indent=2)
        logger.info(f"ë©”íŠ¸ë¦­ ì €ì¥ ì™„ë£Œ: {metrics_path}")

        # ë³€ìˆ˜ ë§¤í•‘ ì €ì¥
        mapping_path = "models/variable_mapping.json"
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.processor.variable_mapping, f, indent=2, ensure_ascii=False)
        logger.info(f"ë³€ìˆ˜ ë§¤í•‘ ì €ì¥ ì™„ë£Œ: {mapping_path}")

    def load_model(self, model_path: str = "models/quality_predictor.pkl"):
        """ëª¨ë¸ ë¡œë“œ"""
        with open(model_path, 'rb') as f:
            self.model = pickle.load(f)
        logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")

        # Scaler ë¡œë“œ
        self.processor.load_scaler("models/scaler.pkl")

        return self.model


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("=" * 70)
    logger.info("SmartFlow ML Model Training")
    logger.info("=" * 70)

    # í•™ìŠµê¸° ì´ˆê¸°í™”
    trainer = ModelTrainer()

    # ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
    model, metrics = trainer.train_xgboost(
        n_estimators=300,    # 150 â†’ 300 (ë” ë§ì€ íŠ¸ë¦¬)
        max_depth=12,        # 8 â†’ 12 (ë” ê¹Šì€ í•™ìŠµ)
        learning_rate=0.03   # 0.05 â†’ 0.03 (ë” ëŠë¦° í•™ìŠµ, ê³¼ì í•© ë°©ì§€)
    )

    # ëª¨ë¸ ì €ì¥
    trainer.save_model()

    # ì„±ëŠ¥ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ¯ ìµœì¢… ì„±ëŠ¥ ìš”ì•½")
    print("=" * 70)
    print(f"âœ… Test RÂ² Score: {metrics['test']['r2']:.4f} (ëª©í‘œ: >0.90)")
    print(f"âœ… Test MAE: {metrics['test']['mae']:.4f}")
    print(f"âœ… Test MAPE: {metrics['test']['mape']:.2f}%")

    if metrics['test']['r2'] >= 0.90:
        print("\nğŸ‰ ëª©í‘œ ë‹¬ì„±! ëª¨ë¸ì´ 92% ì´ìƒì˜ ì •í™•ë„ë¡œ í’ˆì§ˆì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    else:
        print(f"\nâš ï¸  í˜„ì¬ RÂ²: {metrics['test']['r2']:.2%} (ëª©í‘œ: 90%)")

    print("=" * 70)

    logger.info("í•™ìŠµ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
